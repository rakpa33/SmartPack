<?xml version="1.0" encoding="UTF-8"?>
<document>
  <metadata>
    <title>SmartPack Testing Guidelines (Legacy & Project-Specific)</title>
    <purpose>Contains legacy patterns and SmartPack-specific testing guidance. Project-specific test utilities and helpers, troubleshooting existing test issues, legacy pattern explanations during migration.</purpose>
    <lastUpdated>2025-08-08</lastUpdated>
    <documentType>testing-guidelines</documentType>
    <note>This document contains legacy patterns. For current testing standards, see TESTING_STANDARDS.md first.</note>
  </metadata>

  <content>
    <section title="Important Notice">
      <emphasis>NOTE</emphasis>: This document contains legacy patterns and SmartPack-specific guidance. For current testing standards, see `TESTING_STANDARDS.md` first.

      <subsection title="Usage Priority">
        <list type="ordered">
          <item>TESTING_STANDARDS.md patterns (Required)</item>
          <item>SmartPack-specific guidelines below (As needed)</item>
          <item>Legacy patterns in this file (Only when migrating old tests)</item>
        </list>
      </subsection>
    </section>

    <section title="SmartPack Testing Cheatsheet">
      <description>A quick reference for creating, organizing, and running tests in SmartPack. Applies to Vitest + React Testing Library, Supertest, and Playwright.</description>

      <subsection title="Testing Protocol & Best Practices">
        <list>
          <item>When a test fails, always validate the behavior live in the app (via `npm run dev`) before assuming the test is incorrect</item>
          <item>Cross-check test and implementation against external best practices (React Testing Library, accessibility, E2E patterns, etc.)</item>
          <item>Only update or relax tests if the live UI matches the intended acceptance criteria and best practices</item>
          <item>Checklist features (add, check, uncheck, remove, persist) are fully covered by E2E/integration tests</item>
        </list>
      </subsection>
    </section>

    <section title="Test Types & Tools">
      <table>
        <header>
          <column>Layer</column>
          <column>Purpose</column>
          <column>Tools/Notes</column>
        </header>
        <row>
          <cell>Unit</cell>
          <cell>Test a single function/component</cell>
          <cell>Vitest, React Testing Library (RTL)</cell>
        </row>
        <row>
          <cell>Integration</cell>
          <cell>Test collaboration of multiple units</cell>
          <cell>Supertest, nock/msw, in-process server</cell>
        </row>
        <row>
          <cell>E2E</cell>
          <cell>Simulate real user flows (browser→API)</cell>
          <cell>Playwright</cell>
        </row>
      </table>
    </section>

    <section title="Additional Standards & Clarifications">
      <list>
        <item><strong>Integration/E2E Placement</strong>: All Supertest and Playwright tests should live in `__tests__/api/` and `__tests__/e2e/` respectively. Colocation with source code is not allowed for these layers</item>
        <item><strong>Factories & Utilities</strong>: All factory functions and test utilities (e.g., `renderWithProviders`) must be imported from `tests/factories/` and `tests/testing-utils.tsx`. Local helpers within test files are allowed only for one-off, file-specific logic; reusable helpers must be promoted and documented</item>
        <item><strong>Accessibility Checks</strong>: For page-level components, at least one axe-core check is required per component (not per test file). Additional checks are encouraged for complex flows</item>
        <item><strong>Coverage Enforcement</strong>: The 80% threshold is enforced globally and per-PR. PRs should not drop coverage below 80% for any file touched, unless explicitly approved</item>
        <item><strong>Test Data Consistency</strong>: Factories should generate valid, production-like data by default. Edge-case/minimal data is allowed for negative/edge-case tests, but must be clearly documented in the test</item>
        <item><strong>Skipped/Quarantined Tests</strong>: Skipped tests must be reviewed during the quarterly audit and re-enabled or removed as appropriate. All skips must reference a GitHub issue ID</item>
        <item><strong>E2E Canonical Journeys</strong>: Any new critical user journey must be added to `__tests__/e2e/README.md` via PR and reviewed by at least one test lead</item>
        <item><strong>Assertion Messages</strong>: Custom assertion messages should be concise and explain the intent, e.g., `expect(x).toBe(y) /* explains why */`. Use a consistent, descriptive style</item>
        <item><strong>Test Review</strong>: PRs that make major changes to test structure/utilities must be reviewed by a designated "test lead" or equivalent</item>
        <item><strong>Test Audit</strong>: The quarterly "test hygiene" audit must be tracked in GitHub Projects and use a dedicated issue template for documentation</item>
      </list>
    </section>

    <section title="Integration Testing Best Practices">
      <subsection title="Handling Navigation and Async UI Updates">
        <description>When testing navigation between components (e.g., TripForm → MainLayout):</description>
        
        <list type="ordered">
          <item><strong>Use proper query methods</strong>:
            <list>
              <item>Use `findByTestId` instead of `waitFor` + `getByTestId` for elements that appear asynchronously</item>
              <item>Use `findBy*` queries over `waitFor` + `getBy*` for async elements</item>
              <item>Use data-testid attributes for reliable element selection</item>
            </list>
          </item>

          <item><strong>Configure waitFor properly</strong>:
            <code language="tsx"><![CDATA[await waitFor(
  () => {
    expect(screen.queryByTestId('element-id')).not.toBeInTheDocument();
  },
  {
    timeout: 5000, // Increase timeout for slower transitions
    onTimeout: (error) => {
      console.error('Timeout error details:', error);
      console.log('Current DOM:', document.body.innerHTML);
      return error;
    }
  }
);]]></code>
          </item>

          <item><strong>Form interaction best practices</strong>:
            <list>
              <item>Add explicit waits between form interactions (`await userEvent.clear/type/click`)</item>
              <item>Wrap test logic in try/catch for better error messages</item>
              <item>Verify input values after typing with console.log</item>
              <item>Tab out after typing to trigger blur events (important for geocoding)</item>
              <item>Use proper element type casting (e.g., `as HTMLInputElement`)</item>
            </list>
          </item>

          <item><strong>Test setup and environment</strong>:
            <list>
              <item>Initialize localStorage with mock data in `beforeEach` for consistent test behavior</item>
              <item>Mock browser APIs like console.error to filter irrelevant warnings</item>
              <item>Add cleanup in the return function of `beforeEach`</item>
            </list>
          </item>

          <item><strong>Debugging techniques</strong>:
            <list>
              <item>Add strategic console.log statements at critical points</item>
              <item>Log the entire DOM at key transitions</item>
              <item>Log specific element states before assertions</item>
              <item>Use more descriptive test names that explain the expected behavior</item>
            </list>
          </item>
        </list>
      </subsection>

      <subsection title="React Testing Library Query Priority">
        <description>Follow this order for element queries (best to worst):</description>
        <list type="ordered">
          <item>`getByRole` / `findByRole` (with name option)</item>
          <item>`getByLabelText` (for form controls)</item>
          <item>`getByText` (for non-interactive elements)</item>
          <item>`getByTestId` (when semantic queries aren't possible)</item>
        </list>

        <subsection title="Avoid using">
          <list>
            <item>`container.querySelector` (breaks accessibility testing)</item>
            <item>`queryBy*` variants except for checking non-existence</item>
          </list>
        </subsection>
      </subsection>
    </section>

    <section title="General Principles">
      <list>
        <item><strong>Test Pyramid</strong>: Many unit, some integration, few E2E</item>
        <item><strong>AAA Pattern</strong>: Arrange → Act → Assert</item>
        <item><strong>Isolation</strong>: No network/time dependencies in unit tests</item>
        <item><strong>Descriptive Names</strong>: One logical assertion per test</item>
        <item><strong>Fail Fast</strong>: Run unit/integration on every push; E2E on PR/nightly</item>
        <item><strong>Naming</strong>: Name test files with `.test.ts(x)` or `.spec.ts(x)` suffixes</item>
      </list>

      <subsection title="Test Location">
        <list>
          <item>Unit tests should be placed in `src/__tests__/` directory</item>
          <item>Integration tests belong in `src/__tests__/integration/`</item>
          <item>E2E tests belong in the `playwright/` directory</item>
        </list>
      </subsection>

      <subsection title="Test Data">
        <list>
          <item>Use fixtures or factories for test data</item>
          <item>Mock external services (APIs, network calls) using MSW, nock, or vi.mock</item>
        </list>
      </subsection>

      <subsection title="Accessibility">
        <description>Prefer accessible queries and check for a11y issues in UI tests.</description>
      </subsection>
    </section>

    <section title="Unit Test Best Practices (Vitest + RTL)">
      <list>
        <item>Use `describe` blocks per component or util</item>
        <item>Mock fetches with `vi.mock` or `msw`</item>
        <item>Prefer RTL queries (`getByRole`, `findByText`)</item>
        <item>Use `data-testid` for critical elements that need reliable selection in tests</item>
        <item>Keep tests under 100 ms each</item>
        <item>Keep tests colocated with the code they test when possible</item>
        <item>For components that use context, ensure the test provides the necessary context wrapper</item>
        <item>For asynchronous operations, use appropriate waiting mechanisms with reasonable timeouts</item>
      </list>

      <subsection title="Example">
        <code language="typescript"><![CDATA[import { render, screen, waitFor } from '@testing-library/react';
import { TripFormProvider } from '../hooks/TripFormContext';
import TripForm from './TripForm';

describe('TripForm', () => {
  it('prefills default dates', () => {
    render(
      <TripFormProvider>
        <TripForm />
      </TripFormProvider>
    );
    expect(screen.getByLabelText(/start date/i)).toHaveValue(/*…*/);
  });

  it('displays weather data after submission', async () => {
    render(
      <TripFormProvider>
        <TripForm />
      </TripFormProvider>
    );
    
    // Fill and submit form...
    await waitFor(
      () => {
        const weatherElement = screen.getByTestId('weather-display');
        expect(weatherElement).toBeInTheDocument();
      },
      { timeout: 3000 } // Use appropriate timeout
    );
  });
});]]></code>
      </subsection>
    </section>

    <section title="Integration Test Best Practices (Supertest)">
      <list>
        <item>Spin up app with `app.listen()` or in-process serverless wrapper</item>
        <item>Stub LLM/weather calls with `nock` or `msw-node`</item>
        <item>Use `beforeEach/afterEach` to reset mocks</item>
        <item>Test auth headers and error cases</item>
        <item>Place integration tests in `__tests__/integration` or similar</item>
      </list>

      <subsection title="Example">
        <code language="typescript"><![CDATA[import request from 'supertest';
import app from '../app';

describe('/generate route', () => {
  it('returns 200 and list JSON', async () => {
    const res = await request(app)
      .post('/generate')
      .send({ prompt: 'test' })
      .expect(200);
      
    expect(res.body.items.length).toBeGreaterThan(0);
  });
});]]></code>
      </subsection>
    </section>

    <section title="E2E Test Best Practices (Playwright)">
      <list>
        <item>Keep under 5–10 critical user journeys. List and review these regularly</item>
        <item>Use test IDs or ARIA roles for selectors</item>
        <item>Run headless in CI, headed locally</item>
        <item>Record videos only on failure</item>
        <item>Clear localStorage and reset state between specs</item>
        <item>Place E2E tests in `__tests__/e2e` or similar</item>
      </list>

      <subsection title="Critical user journeys must include">
        <list>
          <item>Form completion flows that involve API calls (like geocoding)</item>
          <item>Validation flows that depend on transformed data</item>
          <item>Multi-step processes where state changes across screens</item>
        </list>
      </subsection>

      <subsection title="Example">
        <code language="typescript"><![CDATA[test('user completes trip wizard with geocoded city', async ({ page }) => {
  await page.goto(process.env.APP_URL);
  
  await page.getByRole('textbox', { name: /name/i }).fill('Alice');
  
  // Type a simple city name and ensure it gets geocoded properly
  await page.getByTestId('destination-input-0').fill('paris');
  await page.getByTestId('destination-input-0').blur();
  
  // Verify the geocoded format appears
  await expect(page.getByTestId('destination-input-0')).toHaveValue(
    /Paris.*France/
  );
  
  // Continue flow and verify navigation succeeds
  await page.getByText('Next').click();
  await expect(page.getByText('Packing Checklist')).toBeVisible();
});]]></code>
      </subsection>
    </section>

    <section title="Running Tests in CI">
      <list>
        <item><strong>Unit</strong>: `npm run test:unit` (Vitest)</item>
        <item><strong>Integration</strong>: `npm run test:int` (Supertest)</item>
        <item><strong>E2E</strong>: `npm run test:e2e` (Playwright)</item>
        <item>Run unit/integration on every push; E2E on PRs or nightly</item>
        <item>Fail builds on test or lint errors</item>
      </list>

      <subsection title="GitHub Actions Example">
        <code language="yaml"><![CDATA[name: CI
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 18
      - run: npm ci
      - run: npm run test:unit
      - run: npm run test:int
      - run: npm run test:e2e]]></code>
      </subsection>
    </section>

    <section title="Coverage & Quality Gates">
      <list>
        <item>Vitest coverage (`--coverage`) ≥ 80% lines/branches. Exceptions must be documented</item>
        <item>Lint: `eslint . --max-warnings=0` on every PR</item>
        <item>Review and update tests regularly; remove obsolete tests</item>
        <item>Fail CI if thresholds not met</item>
      </list>
    </section>

    <section title="Debugging & Maintenance">
      <list>
        <item>Reproduce failures locally with `--watch` before debugging</item>
        <item>Use clear assertion messages</item>
        <item>Check mocks (did you override?)</item>
        <item>Isolate: comment out unrelated tests</item>
        <item>Document tricky bugs/fixes in `DEVLOG.md`</item>
        <item>Remove or update flaky/obsolete tests as needed</item>
      </list>
    </section>

    <section title="localStorage and State Management in Tests">
      <emphasis>Critical for SmartPack</emphasis>: Components depend heavily on localStorage for state persistence.

      <subsection title="Comprehensive cleanup in beforeEach">
        <code language="tsx"><![CDATA[beforeEach(() => {
  // Clear ALL localStorage - critical for preventing test contamination
  window.localStorage.clear();
  
  // Explicitly remove known keys for extra safety
  window.localStorage.removeItem('tripForm');
  window.localStorage.removeItem('smartpack_checklist');
  window.localStorage.removeItem('smartpack_categories');
  window.localStorage.removeItem('theme');
});]]></code>
      </subsection>

      <subsection title="Set up required state before component initialization">
        <code language="tsx"><![CDATA[beforeEach(() => {
  // Set up trip form data for MainLayout tests (required for conditional rendering)
  const tripData = {
    tripName: 'Test Trip',
    destinations: ['Paris'],
    travelModes: ['Car'],
    preferences: ['No special needs'],
    startDate: '2025-08-01',
    endDate: '2025-08-10',
    step: 2, // Critical: MainLayout only renders when step === 2
  };
  window.localStorage.setItem('tripForm', JSON.stringify(tripData));
});]]></code>
      </subsection>

      <subsection title="Understanding component conditional rendering">
        <list>
          <item>`TripForm`: Automatically navigates to `/MainLayout` when `state.step === 2`</item>
          <item>`MainLayout`: Only renders content when `state.step >= 2`, otherwise shows loading</item>
          <item><strong>Test Impact</strong>: Set up proper state before rendering to avoid unexpected behavior</item>
        </list>
      </subsection>
    </section>

    <section title="Direct Route Testing vs Full Navigation">
      <emphasis>Prefer targeted testing over complex navigation flows</emphasis>

      <code language="tsx"><![CDATA[// Complex - requires full form flow
render(
  <MemoryRouter initialEntries={['/']}>
    <App />
  </MemoryRouter>
);
await fillOutComplexForm(); // Flaky, hard to debug

// Direct - test specific functionality  
render(
  <MemoryRouter initialEntries={['/MainLayout']}>
    <App />
  </MemoryRouter>
);
// Component renders immediately with proper localStorage setup]]></code>

      <subsection title="When to use each approach">
        <list>
          <item><strong>Direct Route Testing</strong>: For testing specific page/component functionality</item>
          <item><strong>Full Navigation Testing</strong>: For critical user journey validation (fewer tests, more comprehensive)</item>
        </list>
      </subsection>
    </section>

    <section title="Async Element Handling Best Practices">
      <description>Updated patterns based on SmartPack findings:</description>

      <code language="tsx"><![CDATA[// Unreliable
const element = await waitFor(() => screen.getByTestId('element'));

// More reliable for async elements
const element = await screen.findByTestId('element', {}, { timeout: 5000 });

// Best for checking element removal
await waitFor(
  () => {
    expect(screen.queryByText('Item')).not.toBeInTheDocument();
  },
  { timeout: 2000 }
);

// Problematic - requires element to exist first
await waitForElementToBeRemoved(() => screen.queryByText('Item'));]]></code>
    </section>

    <section title="Context-Dependent Component Testing">
      <subsection title="Testing Components with Multiple Context Dependencies">
        <description>When testing components that depend on multiple React contexts (e.g., SuggestionsPanel with TripForm and PackingList contexts):</description>

        <list type="ordered">
          <item><strong>Provider Wrapping Strategy</strong>:
            <code language="tsx"><![CDATA[const renderWithProviders = (component: ReactElement) => {
  return render(
    <TripFormProvider>
      <PackingListProvider>{component}</PackingListProvider>
    </TripFormProvider>
  );
};]]></code>
          </item>

          <item><strong>Mock Context Values</strong>:
            <code language="tsx"><![CDATA[vi.mock('../hooks/useTripForm', () => ({
  useTripForm: vi.fn(() => ({
    state: mockTripState,
    dispatch: vi.fn(),
  })),
}));]]></code>
          </item>

          <item><strong>Watch Mode vs Run Mode</strong>:
            <list>
              <item>Tests with complex context dependencies may hang in watch mode</item>
              <item>Use `npm test -- --run` for final verification</item>
              <item>Always test both isolated components and integration flows</item>
            </list>
          </item>

          <item><strong>API Integration Testing</strong>:
            <list>
              <item>Mock API calls in unit tests: `vi.mock('../services/apiService')`</item>
              <item>Test error handling scenarios with rejected promises</item>
              <item>Verify loading states and user feedback during API calls</item>
            </list>
          </item>
        </list>
      </subsection>

      <subsection title="Test Execution Best Practices">
        <list type="ordered">
          <item><strong>For CI/CD and Final Verification</strong>:
            <code>npm test -- --run --no-watch</code>
          </item>

          <item><strong>For Specific Component Testing</strong>:
            <code>npm test -- --run src/__tests__/ComponentName.test.tsx</code>
          </item>

          <item><strong>Current Known Issues</strong>:
            <list>
              <item>SuggestionsPanel tests may hang in watch mode but pass in run mode</item>
              <item>Always verify test results with `--run` flag before considering them final</item>
            </list>
          </item>

          <item><strong>Test Coverage Verification</strong>:
            <list>
              <item>Aim for >95% test pass rate</item>
              <item>Document any consistently failing tests in TROUBLESHOOTING.md</item>
              <item>Current project status: 69/71 tests passing (97% success rate)</item>
            </list>
          </item>
        </list>
      </subsection>
    </section>

    <section title="Bottom Line">
      <list>
        <item>Fast, focused unit tests catch most regressions</item>
        <item>Selective integration tests ensure routes/services work together</item>
        <item>Lean E2E tests prove the happy path in production-like conditions</item>
        <item>Automate everything in CI, use clear fail messages, and log major issues in `DEVLOG.md`</item>
        <item>Prioritize accessibility and maintainability in all tests</item>
      </list>
    </section>
  </content>
</document>