<?xml version="1.0" encoding="UTF-8"?>
<document>
  <metadata>
    <title>Environment Variables for SmartPack</title>
    <purpose>Complete environment configuration reference for all deployment contexts</purpose>
    <lastUpdated>2025-01-31</lastUpdated>
    <documentType>environment-configuration</documentType>
  </metadata>
  
  <documentation-header>
    <comment>
      This file documents environment variables, configuration settings, deployment procedures, and development environment setup for SmartPack. Keep this comment at the top; do not overwrite or remove it when updating the document. 
      
      DOCUMENT PURPOSE:
      - Complete environment configuration reference for all deployment contexts 
      - Development environment setup with enhanced AI and professional UI requirements 
      - Production deployment procedures for AWS Lambda and static hosting 
      - Environment variable documentation with purpose and usage examples 
      - Port configuration and service startup procedures 
      - Troubleshooting guide for environment-specific issues 
      
      ENVIRONMENT CONTEXTS:
      - Development: Local Vite + Express + Ollama AI setup with enhanced features 
      - Testing: CI/CD environment configuration for comprehensive test suite 
      - Production: AWS Lambda backend + static frontend deployment 
      - Hybrid: Local development with cloud service integration 
      
      WHEN TO UPDATE:
      1. NEW ENVIRONMENT VARIABLES: API URLs, service endpoints, configuration flags 
      2. PORT CHANGES: Service port updates, proxy configurations, development server changes 
      3. DEPLOYMENT UPDATES: AWS configuration, serverless framework changes, hosting updates 
      4. SERVICE INTEGRATION: New external APIs, enhanced AI features, professional UI dependencies 
      5. TESTING ENVIRONMENT: CI/CD configuration, test database setup, mock service configuration 
      6. SECURITY UPDATES: API keys, authentication methods, CORS configuration 
      
      UPDATE GUIDELINES:
      - Document both development and production values for each environment variable 
      - Include clear purpose statements for each configuration setting 
      - Provide verification steps and expected outputs for environment validation 
      - Cross-reference with COMMANDS.md for startup procedures and TROUBLESHOOTING.md for common issues 
      - Maintain current status sections reflecting enhanced AI and professional UI features 
      - Include specific version requirements and compatibility notes 
      
      CONFIGURATION CATEGORIES:
      - Core Services: Frontend (Vite), Backend (Express/Lambda), AI (Ollama) 
      - Development Tools: Testing frameworks, build tools, development servers 
      - External APIs: Weather services, geocoding, enhanced AI integration 
      - Deployment: AWS credentials, serverless configuration, static hosting 
      - Security: CORS settings, API authentication, environment isolation 
      
      VERIFICATION PROCEDURES:
      - All required services accessible on documented ports 
      - Enhanced AI integration functional with intelligent responses 
      - Professional UI features rendering correctly (Heroicons, accessibility) 
      - Complete test suite passing in environment 
      - Production deployment health checks successful 
      
      HOW TO USE FOR AI ASSISTANCE:
      - Reference this document for complete environment setup before troubleshooting connectivity issues 
      - Use verification procedures to systematically diagnose environment problems 
      - Check current integration status to understand implemented features and deployment readiness 
      - Cross-reference with ONBOARDING.md for step-by-step setup and COMMANDS.md for specific command syntax 
      - Validate that environment configuration aligns with current project dependencies and enhanced features 
      - Use troubleshooting sections to resolve common environment-specific issues
    </comment>
  </documentation-header>

  <section name="Development Environment">
    <environment-variables>
      <frontend>
        <variable name="NODE_ENV" value="development"/>
        <variable name="VITE_API_URL" value="http://localhost:3000"/>
      </frontend>
      
      <backend>
        <variable name="PORT" value="3000"/>
        <variable name="NODE_ENV" value="development"/>
      </backend>
      
      <ollama-ai>
        <variable name="OLLAMA_HOST" value="http://localhost:11434" description="Local Ollama service"/>
        <variable name="OLLAMA_MODEL" value="llama3.1:8b" description="AI model for packing recommendations"/>
      </ollama-ai>
    </environment-variables>
  </section>

  <section name="Required Development Setup">
    <prerequisites>
      <requirement>Node.js v20.14.0 or later</requirement>
      <requirement>npm 10.7.0 or later</requirement>
      <requirement>
        <name>Ollama installed and running locally</name>
        <setup>
          <step>Download from: https://ollama.ai/</step>
          <step>Install llama3.1:8b model: ollama pull llama3.1:8b</step>
          <step>Start service: ollama serve (runs on port 11434)</step>
        </setup>
      </requirement>
    </prerequisites>
    
    <local-development-ports>
      <port service="Frontend (Vite)" url="http://localhost:5173"/>
      <port service="Backend (Express)" url="http://localhost:3000"/>
      <port service="Ollama AI Service" url="http://localhost:11434"/>
      <port service="Health Check" url="http://localhost:3000/health"/>
    </local-development-ports>
    
    <startup-commands>
      <terminal number="1">
        <description>Start Ollama service (if not running as service)</description>
        <command>ollama serve</command>
      </terminal>
      <terminal number="2">
        <description>Start backend with AI integration</description>
        <command>npm run lambda:dev</command>
      </terminal>
      <terminal number="3">
        <description>Start frontend</description>
        <command>npm run dev</command>
      </terminal>
    </startup-commands>
    
    <installation-steps>
      <step>Install dependencies with legacy peer deps for compatibility: npm install --legacy-peer-deps</step>
      <step>Verify Ollama model availability: ollama list (Should show llama3.1:8b)</step>
      <step>Test Ollama integration: ollama run llama3.1:8b "Generate a packing list for a 3-day business trip"</step>
    </installation-steps>
    
    <verification-steps>
      <step>Ollama Service: Verify http://localhost:11434 responds</step>
      <step>Backend Health: Visit http://localhost:3000/health</step>
      <step>Frontend Running: Visit http://localhost:5173</step>
      <step>AI Integration Test: Generate packing list and verify real AI responses (not mock data)</step>
      <step>Professional UI Verification: Confirm Heroicons vector icons display properly</step>
      <step>Testing Suite: Run npm test -- --run and npx playwright test</step>
      <step>Complete AI Workflow: Plan trip → Generate AI checklist → Custom AI suggestions</step>
    </verification-steps>
  </section>

  <section name="Production Environment">
    <environment-variables>
      <frontend>
        <variable name="NODE_ENV" value="production"/>
        <variable name="VITE_API_URL" value="https://your-deployed-api-gateway-url.amazonaws.com"/>
      </frontend>
      
      <backend>
        <variable name="NODE_ENV" value="production"/>
        <variable name="AWS_REGION" value="us-east-2"/>
      </backend>
    </environment-variables>
  </section>

  <section name="AWS Deployment Variables">
    <description>Required for serverless deployment:</description>
    <variables>
      <variable name="AWS_PROFILE" value="default"/>
      <variable name="AWS_REGION" value="us-east-2"/>
      <variable name="AWS_ACCESS_KEY_ID" value="your-access-key"/>
      <variable name="AWS_SECRET_ACCESS_KEY" value="your-secret-key"/>
    </variables>
  </section>

  <section name="How to Use">
    <step>Copy .env.example to .env and fill in the values for your environment.</step>
    <step>Reference this file for variable descriptions and usage.</step>
  </section>

  <section name="Deployment Steps">
    <backend-deployment>
      <prerequisites>
        <step>Ensure AWS CLI is configured: aws configure list</step>
        <step>Install serverless dependencies: npm install -D serverless@3 serverless-esbuild serverless-offline</step>
      </prerequisites>
      
      <build-and-deploy>
        <step>Build the lambda function: npm run lambda:build</step>
        <step>Deploy to AWS: npm run lambda:deploy</step>
      </build-and-deploy>
      
      <update-frontend-configuration>
        <step>After successful deployment, update src/services/apiService.ts</step>
        <step>Replace http://localhost:3000 with your deployed API Gateway URL</step>
        <step>Update the production URL in serverless.yml custom.corsOrigin.prod</step>
      </update-frontend-configuration>
    </backend-deployment>
    
    <frontend-deployment>
      <step>Build for production: npm run build</step>
      <step>Deploy to your hosting service (Vercel, Netlify, S3, etc.)</step>
    </frontend-deployment>
  </section>

  <section name="Current Integration Status">
    <status-item>Enhanced AI Backend: Complete intelligent system with 7-aspect trip analysis</status-item>
    <status-item>Professional UI: Heroicons React vector icon system with ARIA accessibility</status-item>
    <status-item>Comprehensive Testing: Unit, integration, and E2E test coverage following external best practices</status-item>
    <status-item>Backend API: Lambda function with Express.js server and intelligent recommendations</status-item>
    <status-item>Frontend Integration: TripForm calls enhanced AI backend for context-aware packing lists</status-item>
    <status-item>AI Suggestions Panel: Smart prompt refinement with intelligent backend integration</status-item>
    <status-item>Local Development: Enhanced backend runs on http://localhost:3000</status-item>
    <status-item>Health Monitoring: Backend health check endpoint available</status-item>
    <status-item>CORS Configuration: Properly configured for local development</status-item>
    <status-item>Error Handling: Graceful API error handling in frontend</status-item>
    <status-item>Test Suite: Vitest + React Testing Library + Playwright comprehensive coverage</status-item>
    <status-item>Production Deployment: Ready for AWS Lambda deployment with enhanced AI</status-item>
    <status-item>Production CORS: Update production URLs after deployment</status-item>
  </section>

  <section name="Troubleshooting Environment Issues">
    <subsection name="Enhanced AI Backend Connection Problems">
      <commands>
        <command description="Check if enhanced AI backend is running">netstat -ano | findstr ":3000"</command>
        <command description="Test enhanced backend health">curl http://localhost:3000/health</command>
        <command description="OR visit in browser">http://localhost:3000/health</command>
        <command description="Restart enhanced AI backend if needed">
          <step>taskkill /F /IM node.exe</step>
          <step>npm run lambda:dev</step>
        </command>
        <command description="Test intelligent recommendations"><![CDATA[
curl -X POST http://localhost:3000/api/generate-suggestions ^
  -H "Content-Type: application/json" ^
  -d "{\"prompt\":\"4-day business trip to Chicago\"}"
        ]]></command>
      </commands>
    </subsection>
    
    <subsection name="Professional UI Icon Issues">
      <checklist>
        <item>Verify Heroicons React package is installed: npm list @heroicons/react</item>
        <item>Check that vector icons render properly (not emoji)</item>
        <item>Ensure ARIA accessibility attributes are present</item>
        <item>Test icon responsiveness across device sizes</item>
      </checklist>
    </subsection>
    
    <subsection name="Comprehensive Testing Environment">
      <commands>
        <command description="Run complete test suite">npm test -- --run</command>
        <command description="Run E2E tests specifically">npx playwright test</command>
        <command description="Run enhanced AI unit tests">npm test -- enhancedAI.unit.test.ts</command>
        <command description="Run integration tests">npm test -- enhancedAI.integration.test.tsx</command>
        <command description="Run E2E tests">npx playwright test enhancedAI.e2e.test.ts</command>
      </commands>
    </subsection>
    
    <subsection name="Frontend API Connection Issues">
      <checklist>
        <item>Verify VITE_API_URL points to http://localhost:3000 in development</item>
        <item>Check browser developer console for CORS or network errors</item>
        <item>Ensure both enhanced AI frontend and backend are running simultaneously</item>
        <item>Test Enhanced AI: Verify intelligent recommendations appear (not repetitive static suggestions)</item>
        <item>Professional UI Check: Confirm Heroicons vector icons load properly</item>
        <item>Test Coverage: Run test suite to validate API integration functionality</item>
      </checklist>
    </subsection>
  </section>
</document>